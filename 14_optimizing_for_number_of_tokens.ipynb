{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to check how many tokens we need for optimal language classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "raw_dir = \"/home/peterr/macocu/taskB/data/raw\"\n",
    "interim_dir = \"/home/peterr/macocu/taskB/data/interim\"\n",
    "final_dir = \"/home/peterr/macocu/taskB/data/final\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import parse\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def get_N_tokens(N=5000, path=\"/home/peterr/macocu/taskB/task4/toy_tokens.csv\") -> set:\n",
    "\n",
    "    df = pd.read_csv(\"toy_tokens.csv\", index_col=0)\n",
    "    NUM_FEATS = N\n",
    "\n",
    "    for column in df.columns:\n",
    "        new_column_name = column + \"_f\"\n",
    "        corpus_size = df[column].sum()\n",
    "        df[new_column_name] = df[column] * 1e6 / corpus_size\n",
    "\n",
    "    N = 1\n",
    "\n",
    "    df[\"HR_SR\"] = (df[\"hrwac_head_pp_f\"] + N) / (df[\"srwac_head_pp_f\"] + N)\n",
    "    df[\"SR_HR\"] = (df[\"srwac_head_pp_f\"] + N) / (df[\"hrwac_head_pp_f\"] + N)\n",
    "\n",
    "    df[\"HR_CNR\"] = (df[\"hrwac_head_pp_f\"] + N) / (df[\"cnrwac_head_pp_f\"] + N)\n",
    "    df[\"CNR_HR\"] = (df[\"cnrwac_head_pp_f\"] + N) / (df[\"hrwac_head_pp_f\"] + N)\n",
    "\n",
    "    df[\"HR_BS\"] = (df[\"hrwac_head_pp_f\"] + N) / (df[\"bswac_head_pp_f\"] + N)\n",
    "    df[\"BS_HR\"] = (df[\"bswac_head_pp_f\"] + N) / (df[\"hrwac_head_pp_f\"] + N)\n",
    "\n",
    "    df[\"BS_SR\"] = (df[\"bswac_head_pp_f\"] + N) / (df[\"srwac_head_pp_f\"] + N)\n",
    "    df[\"SR_BS\"] = (df[\"srwac_head_pp_f\"] + N) / (df[\"bswac_head_pp_f\"] + N)\n",
    "\n",
    "    df[\"BS_CNR\"] = (df[\"bswac_head_pp_f\"] + N) / (df[\"cnrwac_head_pp_f\"] + N)\n",
    "    df[\"CNR_BS\"] = (df[\"cnrwac_head_pp_f\"] + N) / (df[\"bswac_head_pp_f\"] + N)\n",
    "\n",
    "    df[\"CNR_SR\"] = (df[\"cnrwac_head_pp_f\"] + N) / (df[\"srwac_head_pp_f\"] + N)\n",
    "    df[\"SR_CNR\"] = (df[\"srwac_head_pp_f\"] + N) / (df[\"cnrwac_head_pp_f\"] + N)\n",
    "\n",
    "    combos = ['HR_SR', 'SR_HR', 'HR_CNR', 'CNR_HR', 'HR_BS', 'BS_HR',\n",
    "              'BS_SR', 'SR_BS', 'BS_CNR', 'CNR_BS', 'CNR_SR', 'SR_CNR']\n",
    "\n",
    "    important_features = set()\n",
    "\n",
    "    for lang_comb in combos:\n",
    "        s = df.sort_values(lang_comb, ascending=False)[lang_comb]\n",
    "        current_features = s.index[:NUM_FEATS].values\n",
    "        important_features = important_features.union(set(current_features))\n",
    "    try:\n",
    "        important_features.remove(np.nan)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    return important_features\n",
    "\n",
    "\n",
    "def read_and_split_file(path: str) -> List[str]:\n",
    "    texts = list()\n",
    "    chunk = \"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        content = f.readlines()\n",
    "    for line in content:\n",
    "        # Handle splits\n",
    "        if line == \"\\n\":\n",
    "            texts.append(chunk)\n",
    "            chunk = \"\"\n",
    "        # Filter only lowercase alphabetical words:\n",
    "        from utils import is_alpha\n",
    "        line = line.replace(\"\\n\", \" \")\n",
    "        words = [w if is_alpha(w) else \" \" for w in line.split(\" \")]\n",
    "        chunk += \" \".join(words)\n",
    "    return texts\n",
    "\n",
    "\n",
    "texts, labels = list(), list()\n",
    "\n",
    "files = [\n",
    "    \"bswac_tail_pp\",\n",
    "    \"cnrwac_tail_pp\",\n",
    "    \"hrwac_tail_pp\",\n",
    "    \"srwac_tail_pp\"]\n",
    "\n",
    "langs = [\"bs\", \"me\", \"hr\", \"sr\"]\n",
    "\n",
    "for file, lang in zip(files, langs):\n",
    "    full_path = os.path.join(interim_dir, file)\n",
    "    current_texts = read_and_split_file(full_path)\n",
    "    len_cur_texts = len(current_texts)\n",
    "    texts.extend(current_texts)\n",
    "    labels.extend([lang]*len_cur_texts)\n",
    "\n",
    "train = pd.DataFrame(data={\"text\": texts, \"labels\": labels})\n",
    "\n",
    "del texts, labels\n",
    "\n",
    "SETimes = list()\n",
    "for split in [\"train\", \"test\", \"dev\"]:\n",
    "    with open(os.path.join(final_dir, f\"{split}.fasttxt\"), \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        SETimes.extend(lines)\n",
    "\n",
    "p = parse.compile(\"__label__{lang} {text}\")\n",
    "langs = list()\n",
    "texts = list()\n",
    "\n",
    "for line in SETimes:\n",
    "    results = p.parse(line)\n",
    "    if not results:\n",
    "        logging.error(f\"Error parsing line {line}\")\n",
    "        continue\n",
    "    langs.append(results[\"lang\"])\n",
    "    texts.append(results[\"text\"])\n",
    "\n",
    "eval_df = pd.DataFrame(data={\"text\": texts, \"labels\": langs})\n",
    "\n",
    "del texts, langs, SETimes, line, lines, p\n",
    "\n",
    "\n",
    "def get_stats(N: int):\n",
    "    import gc\n",
    "    import time\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    gc.collect()\n",
    "    start = time.time()\n",
    "    vectorizer = CountVectorizer(\n",
    "        vocabulary=get_N_tokens(N), lowercase=True, binary=True)\n",
    "\n",
    "    train_vectors = vectorizer.fit_transform(train.text)\n",
    "    train_labels = train.labels\n",
    "\n",
    "    test_vectors = vectorizer.fit_transform(eval_df.text)\n",
    "    y_true = eval_df.labels\n",
    "\n",
    "    clf = GaussianNB()\n",
    "    train_start = time.time()\n",
    "    clf.fit(train_vectors.toarray(), train_labels)\n",
    "    predict_start = time.time()\n",
    "    y_pred = clf.predict(test_vectors.toarray())\n",
    "    predict_end = time.time()\n",
    "    from sklearn.metrics import f1_score, ConfusionMatrixDisplay, confusion_matrix, accuracy_score\n",
    "    import matplotlib.pyplot as plt\n",
    "    LABELS = [\"hr\", \"bs\", \"sr\",  \"me\"]\n",
    "    LABELS = [\"hr\", \"bs\", \"sr\",  \"me\"]\n",
    "\n",
    "    macro = f1_score(y_true, y_pred, labels=LABELS, average=\"macro\")\n",
    "    micro = f1_score(y_true, y_pred, labels=LABELS,  average=\"micro\")\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=LABELS)\n",
    "    return {\n",
    "        \"N\": N,\n",
    "        \"microF1\": micro,\n",
    "        \"macroF1\": macro,\n",
    "        \"accuracy\": acc,\n",
    "        \"overall_time\": time.time() - start,\n",
    "        \"cm\": cm,\n",
    "        \"vectorizer_fitting\": train_start - start,\n",
    "        \"training_time\": predict_start - train_start,\n",
    "        \"predicting_time\": predict_end - predict_start\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-03 13:01:19,700 - N=100\n",
      "2022-01-03 13:01:20,650 - NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "results = list()\n",
    "Ns = np.logspace(2, 5, 15, dtype=np.int)\n",
    "for N in Ns:\n",
    "    try:\n",
    "        gc.collect()\n",
    "        logging.info(f\"{N=}\")\n",
    "        cur_result = get_stats(N)\n",
    "        logging.info(f\"Done!\")\n",
    "        results.append(cur_result)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"For {N=} got Exception: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f6f5766036ee03d059e365a942add07f79c17033585e9357ee8157d52fe6bb9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
